{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a simple neural network\n",
    "\n",
    "This is an example of implementating a simple neural network manually without dl frameworks (such as tensorflow and pytorch, etc). The network structure is as below:\n",
    "![jupyter](./images/network.png)\n",
    "The caculation in the neural network:\n",
    "![jupyter](./images/network_cal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages \n",
    "All we need is the math and numpy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import e\n",
    "import numpy.random as rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network parameters\n",
    "To make it simple, we ignore the bias (b=0) and only focus on the weight parameters. So this neural network has 8 parameters, which is initialized randomly.\n",
    "- w0 defines the weights of the input of the hidden layer, it contains 2*3 parameters \n",
    "- w1 defines weights of the output of the hidden layer, it contains 2 parameters\n",
    "- h is the hidden layer's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6611529751056572, 0.6611529751056572, 0.6611529751056572], [0.5374184281561075, 0.5374184281561075, 0.5374184281561075]]\n",
      "[0.45861134553660576, 0.8206650584684276]\n",
      "[0.3467988090138425, 0.8889623486404796]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights randomly\n",
    "w0 = [[rnd.rand()] * 3 for i in range(2)]\n",
    "w1 = [rnd.rand() for i in range(2)]\n",
    "h = [rnd.rand() for i in range(2)]\n",
    "print(w0)\n",
    "print(w1)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define activation function: sigmoid\n",
    "![jupyter](./images/sigmoid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# define sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.pow(e, -1 * 2* x))\n",
    "\n",
    "# define sigmoid derivative\n",
    "def d_sigmoid(x):\n",
    "    return sigmoid(2*x) * (1 - sigmoid(2*x)) * 2\n",
    "\n",
    "print(sigmoid(0))\n",
    "print(d_sigmoid(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a single neuron \n",
    "x and w are input vector and weight vector of the neron, so the calculation can be coded as below:  \n",
    "\n",
    "- p = f(h, w1, activate=False)  \n",
    "- y^ = sigmoid(p) = f(h, w1, activate=True)  \n",
    "- q = f(xi, w0[0], activate=False)  \n",
    "- h[0] = sigmoid(q) = f(xi, w0[0], activate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a single neuron\n",
    "def f(x, w, activate=True):\n",
    "    ret = 0\n",
    "    for i in range(len(x)):\n",
    "        ret += x[i] * w[i]\n",
    "    if activate:\n",
    "        return sigmoid(ret)\n",
    "    else:\n",
    "        return ret\n",
    "\n",
    "#print(f([1.0, 0.9, 0.8], w0[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function: logloss\n",
    "Here add a small variable epsilon to y^ incase y^ equals 0.   \n",
    "Parameter `label` is the ground truth and `prob` is the predicted y^.\n",
    "![jupyter](./images/logloss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931469805599654\n",
      "1.0101009080706154\n",
      "-1.0101009080706154\n"
     ]
    }
   ],
   "source": [
    "# define logloss\n",
    "epsilon = 0.0000001\n",
    "def logloss(label, prob):\n",
    "    return -1 * (label * math.log(prob + epsilon, e) + (1 - label) * math.log(1 - prob + epsilon, e))\n",
    "\n",
    "# define losloss derivative\n",
    "def d_logloss(label, prob):\n",
    "    return -1 * label / (prob+epsilon) + (1 - label)/(1 - prob + epsilon)\n",
    "\n",
    "print(logloss(1, 0.5))\n",
    "print(d_logloss(0, 0.01))\n",
    "print(d_logloss(1, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the sample data\n",
    "Let's define the dataset based on below rules:\n",
    "- If the value of xi in all dimentions > 0.5, y=1  \n",
    "- If any of the xi <=0.5, y=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training data set x and y\n",
    "x = [[0.1,0.1, 0.1],\n",
    "     [0.9, 0.9, 0.9],\n",
    "     [0.2,0.1,0.1],\n",
    "     [0.9,0.97,0.89],\n",
    "     [0.1,0.2,0.1],\n",
    "     [0.8,0.9,0.9],\n",
    "     [0.3,0.1,0.4],\n",
    "     [0.9,0.8,0.7],\n",
    "     [0.11,0.22,0.15],\n",
    "     [0.88,0.9,0.9]\n",
    "   ]\n",
    "y = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network\n",
    "### Define feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feed forward \n",
    "def feed_forward(x):\n",
    "    h[0] = f(x, w0[0])\n",
    "    h[1] = f(x, w0[1])\n",
    "    prob = f(h, w1)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define backward propagation\n",
    "![jupyter](./images/backward.png)\n",
    "To make it simple, use batchsize=1. If batchsize > 1, need to sum up d_w for all batch data records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define backward propagation\n",
    "# w = w - lr * d_w\n",
    "def back_propagation(iteration, x, prob, label, LR):\n",
    "    \n",
    "    w00_tmp = [w0[0][i] for i in range(len(w0[0]))]\n",
    "    w01_tmp = [w0[1][i] for i in range(len(w0[1]))]\n",
    "    w1_tmp = [w1[i] for i in range(len(w1))]\n",
    "    # update w0[0]\n",
    "    for i in range(len(w0[0])):\n",
    "        w0[0][i] = w0[0][i] - LR * d_logloss(label, prob) * ( d_sigmoid(f(h, w1_tmp, False))*w1_tmp[0] ) * ( d_sigmoid(f(x, w00_tmp, False))*x[i] )\n",
    "        #print(\"1\", label, d_logloss(label, prob) * ( d_sigmoid(f(h, w1_tmp, False))*w1_tmp[0] ) * ( d_sigmoid(f(x, w00_tmp, False))*x[i] ))\n",
    "    \n",
    "    # update w0[1]\n",
    "    for i in range(len(w0[1])):\n",
    "        w0[1][i] = w0[1][i] - LR * d_logloss(label, prob) * ( d_sigmoid(f(h, w1_tmp, False))*w1_tmp[1] ) * ( d_sigmoid(f(x, w01_tmp, False))*x[i] )\n",
    "        #print(\"2\", label, d_logloss(label, prob) * ( d_sigmoid(f(h, w1_tmp, False))*w1_tmp[1] ) * ( d_sigmoid(f(x, w01_tmp, False))*x[i] ))\n",
    "\n",
    "    # update w[1]\n",
    "    for i in range(len(w1)):\n",
    "        w1[i] = w1[i] - LR * d_logloss(label, prob) * d_sigmoid(f(h, w1_tmp, False)) * h[i]\n",
    "        #print(\"3\", label, d_logloss(label, prob) * d_sigmoid(f(h, w1_tmp, False)) * h[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training\n",
    "Run 100 epoch and print the loss. the loss is reduced during the training and hence we got a trained model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:\n",
      "w0: [[0.6423246656512983, 0.6425862575905961, 0.6436168409544221], [0.488754480452788, 0.48908115207036446, 0.48819181249045596]]\n",
      "w1: [0.1811927657376681, 0.560779820435523]\n",
      "loss: 0.8769063993807341\n",
      "\n",
      "Epoch 10:\n",
      "w0: [[0.38357211213971754, 0.3897357260739514, 0.3832595451841559], [0.1763071217050511, 0.2182758441658364, 0.1567285414667283]]\n",
      "w1: [0.45462153789899573, 0.24526385152143668]\n",
      "loss: 0.8266556537312943\n",
      "\n",
      "Epoch 20:\n",
      "w0: [[0.26763635640782163, 0.3805026795049878, 0.17308846566115746], [-0.34516348277751985, -0.4115679812866288, -0.20130057901126192]]\n",
      "w1: [1.254031925213235, -2.6045413407748526]\n",
      "loss: 0.3815644337165899\n",
      "\n",
      "Epoch 30:\n",
      "w0: [[0.28103721268284404, 0.4763106703940957, 0.07315480374010036], [-0.34226887563116737, -0.575503185141438, 0.04914811610567399]]\n",
      "w1: [1.839289811071246, -3.934247333002802]\n",
      "loss: 0.23111074577590185\n",
      "\n",
      "Epoch 40:\n",
      "w0: [[0.2902508603697472, 0.5043964486216846, 0.03251306107526391], [-0.35282281240372715, -0.6368249780711157, 0.14651047174195442]]\n",
      "w1: [2.135140388008576, -4.5860203227320575]\n",
      "loss: 0.1812426084807945\n",
      "\n",
      "Epoch 50:\n",
      "w0: [[0.3003375358949337, 0.5127175695347952, 0.013840086447696822], [-0.3687095584027383, -0.658724017015227, 0.19185591785827183]]\n",
      "w1: [2.327121451536172, -5.01763874957429]\n",
      "loss: 0.15445122650093185\n",
      "\n",
      "Epoch 60:\n",
      "w0: [[0.309632586567463, 0.5143358814275222, 0.0032864619150518998], [-0.38432757427998776, -0.6660576230142367, 0.2180775008170825]]\n",
      "w1: [2.4702629506755276, -5.340352867017503]\n",
      "loss: 0.13717571446329818\n",
      "\n",
      "Epoch 70:\n",
      "w0: [[0.3178056490965271, 0.513366781626041, -0.0035432749592922428], [-0.398436099946684, -0.6674530531103399, 0.23537954525158994]]\n",
      "w1: [2.584945793522705, -5.598269131913705]\n",
      "loss: 0.12484589237487609\n",
      "\n",
      "Epoch 80:\n",
      "w0: [[0.3249331379808326, 0.5113524315588321, -0.008367326243181813], [-0.41090756281859764, -0.6662605101644451, 0.24780103261866507]]\n",
      "w1: [2.680853892957873, -5.813230789679146]\n",
      "loss: 0.11546388516398602\n",
      "\n",
      "Epoch 90:\n",
      "w0: [[0.33116728810552815, 0.5089416883344227, -0.011983072281983153], [-0.4218901783790756, -0.6639298610728662, 0.2572464166709965]]\n",
      "w1: [2.7633812266615543, -5.9976217471815]\n",
      "loss: 0.10800522162649899\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "EPOCH = 100\n",
    "for epo in range(EPOCH):\n",
    "    loss = 0\n",
    "    for i in range(len(x)):  # batch size=1\n",
    "        prob = feed_forward(x[i])\n",
    "        #print(prob, y[i])\n",
    "        back_propagation(i, x[i], prob, y[i], 0.4)\n",
    "        loss += logloss(y[i], prob)\n",
    "    \n",
    "    if (epo % 10 == 0):\n",
    "        print(\"\\nEpoch {}:\".format(epo))\n",
    "        print(\"w0: \" + str(w0))\n",
    "        print(\"w1: \" + str(w1))\n",
    "        print(\"loss:\", loss/len(x))\n",
    "        #print(feed_forward([0.1, 0.1, 0.1]))\n",
    "        #print(feed_forward([0.9, 0.9, 0.9]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some prediction :)\n",
    "Now the neural network is trained, we could use this network to predict some new coming data records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8802025264184699\n",
      "0.1065727392355342\n"
     ]
    }
   ],
   "source": [
    "# predict \n",
    "print(feed_forward([0.9, 0.8, 0.92]))\n",
    "print(feed_forward([0.11, 0.2, 0.18]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24739257024682754\n"
     ]
    }
   ],
   "source": [
    "print(feed_forward([0.9, 0.1, 0.92]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
